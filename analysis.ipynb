{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Walmart Lite - Analysis\n",
    "\n",
    "Main Objective: Perform analysis on datasets to find optimal top 10 DMAs to launch a \"Walmart-Lite\" store in.\n",
    "\n",
    "First, we'll import the necessary libraries and read the dataset files into DataFrames that Python/Pandas can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Purpose: Read in dataset files into pandas DataFrame\n",
    "datasets = {}\n",
    "files = []\n",
    "\n",
    "def loadFile(path):\n",
    "    global files\n",
    "    global datasets\n",
    "    \n",
    "    fileToLoad = Path('datasets/norteast.xlsx')\n",
    "    fileExists = fileToLoad.is_file()\n",
    "    if not fileExists:\n",
    "        return False\n",
    "    \n",
    "    files.append(path)\n",
    "    \n",
    "    filename,fileExtension = os.path.splitext(path)\n",
    "    \n",
    "    if '.csv' == fileExtension:\n",
    "        df = pd.read_csv(path)\n",
    "    elif '.xlsx' == fileExtension:\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    datasets[ntpath.basename(path)] = df\n",
    "    return True\n",
    "\n",
    "def mapDMAToName(data,searchTerm,isExactMatch = True):\n",
    "    if isExactMatch:\n",
    "        result = data.query(\"DMA == @searchTerm\")\n",
    "        if len(result) >= 1:\n",
    "            return result.values[0][1]\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def mapStoreNumberToDMA(storeNumber):\n",
    "    result = datasets[\"stores data-set-withDMA.xlsx\"].query(\"Store == @storeNumber\")\n",
    "    if len(result) >= 1:\n",
    "        return result.values[0][3]\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def mapDMAToStoreNumber(DMA):\n",
    "    result = datasets[\"stores data-set-withDMA.xlsx\"].iterrows()\n",
    "    if len(result) >= 1:\n",
    "        return result.values[0][0]\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def rankDictionary(dictionaryToRank):\n",
    "    sorted_x = sorted(dictionaryToRank.items(), key=lambda kv: kv[1])\n",
    "    out_dict = OrderedDict(sorted_x)\n",
    "    return out_dict\n",
    "\n",
    "loadFile(\"datasets/modified/Features data set.csv\")\n",
    "loadFile(\"datasets/sales data-set.csv\")\n",
    "loadFile(\"datasets/stores data-set-withDMA.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the restriction in the assignment specification, we need to adjust the timeline of the features dataset accordingly to only include February 5, 2010 to January 11, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose: Adjust features dataset timeline to restriction (Feburary 5, 2010 to January 11, 2012)\n",
    "\n",
    "def adjustFeaturesTimeline(startDate,endDate):\n",
    "    global datasets\n",
    "    startDate = datetime.datetime.strptime(startDate, \"%d/%m/%Y\").date()\n",
    "    endDate = datetime.datetime.strptime(endDate, \"%d/%m/%Y\").date()\n",
    "    \n",
    "    for index,row in datasets[\"Features data set.csv\"].iterrows():\n",
    "        currentDate = datetime.datetime.strptime(row['Date'], \"%d/%m/%Y\").date()\n",
    "        if currentDate > endDate or currentDate < startDate:\n",
    "            datasets[\"Features data set.csv\"] = datasets[\"Features data set.csv\"].drop([index])\n",
    "\n",
    "\n",
    "adjustFeaturesTimeline(\"05/02/2010\",\"01/11/2012\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheets containing consumer info for each area are structured differently than the features and sales sheets, so they will need to be imported differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose: Import consumer info per area for analysis (population count, income, expenditures related to retail)\n",
    "\n",
    "usefulData = [\n",
    "    'Item',\n",
    "    'Number of consumer units (in thousands)',\n",
    "    'Income before taxes',\n",
    "    'Age of reference person',\n",
    "    'Adults 65 and older',\n",
    "    'Earners',\n",
    "    'Vehicles',\n",
    "    'Percent homeowner',\n",
    "    'Average annual expenditures',\n",
    "    'Food',\n",
    "    'Alcoholic beverages',\n",
    "    'Apparel and services',\n",
    "    'Personal care products and services',\n",
    "    'Tobacco products and smoking supplies'\n",
    "]\n",
    "\n",
    "areas = {}\n",
    "areaInfo = {}\n",
    "\n",
    "loadFile(\"datasets/norteast.xlsx\")\n",
    "loadFile(\"datasets/midwest.xlsx\")\n",
    "loadFile(\"datasets/south.xlsx\")\n",
    "loadFile(\"datasets/west.xlsx\")\n",
    "\n",
    "def loadMSA(fileName):\n",
    "    global datasets\n",
    "    global usefulData\n",
    "    global areas\n",
    "    \n",
    "    bufferListOfPlaces = []\n",
    "    bufferDict = {}\n",
    "    for value in datasets[fileName].values:\n",
    "        attribute = value[0]\n",
    "        if attribute in usefulData:\n",
    "            if(attribute == 'Item'): # List of DMAs\n",
    "                bufferListOfPlaces = value[1:]\n",
    "            else:\n",
    "                bufferDict[attribute] = value\n",
    "        \n",
    "    if len(bufferListOfPlaces) > 0:\n",
    "        i = 1\n",
    "        for place in bufferListOfPlaces:\n",
    "            place = place.replace('\\n',' ')\n",
    "            for _,value in bufferDict.items():\n",
    "                attribute = value[0]\n",
    "                attributeValue = value[i]\n",
    "\n",
    "                if place not in areas:\n",
    "                    areas[place] = {}\n",
    "                \n",
    "                areas[place][attribute] = attributeValue\n",
    "            i+=1\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "                \n",
    "loadMSA(\"midwest.xlsx\")\n",
    "loadMSA(\"norteast.xlsx\")\n",
    "loadMSA(\"south.xlsx\")\n",
    "loadMSA(\"west.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings of Overall Spending (Sum of Medians of Stores)\n",
      "San Diego : 8896.69\n",
      "Denver : 15188.0\n",
      "Atlanta : 20756.095\n",
      "Tampa-St. Pete : 22269.079999999998\n",
      "Chicago : 23870.585\n",
      "Philadelphia : 24403.07\n",
      "Dallas-Ft.Worth : 34491.115000000005\n",
      "Cleveland-Akron : 39530.765\n",
      "Houston : 47302.36\n",
      "Los Angeles : 53975.275\n",
      "\n",
      "Rankings of Spending per Capita (Sum of Medians of Stores/Number of Customer Units)\n",
      "Chicago : 6.367187249933315\n",
      "San Diego : 7.151680064308682\n",
      "Los Angeles : 8.517480669086318\n",
      "Philadelphia : 10.013569963069347\n",
      "Atlanta : 10.55752543234995\n",
      "Denver : 10.680731364275669\n",
      "Dallas-Ft.Worth : 12.474182640144667\n",
      "Tampa-St. Pete : 18.573044203502917\n",
      "Houston : 18.674441373864983\n",
      "Cleveland-Akron : 37.576772813688216\n"
     ]
    }
   ],
   "source": [
    "#Purpose: Get a median for all sales in stores\n",
    "\n",
    "loadFile(\"datasets/custom/custom-dma-to-consumerArea.csv\")\n",
    "\n",
    "sales = datasets[\"sales data-set.csv\"].drop(['Dept','Date','IsHoliday'],axis=1)\n",
    "\n",
    "def getSalesByStore():\n",
    "    global sales\n",
    "    \n",
    "    storeSales = {}\n",
    "    \n",
    "    for _,row in sales.iterrows():\n",
    "        storeNumber = int(row['Store'])\n",
    "        weeklySales = row['Weekly_Sales']\n",
    "        if storeNumber in storeSales:\n",
    "            storeSales[storeNumber].append(weeklySales)\n",
    "        else:\n",
    "            storeSales[storeNumber] = [weeklySales]\n",
    "    \n",
    "    return storeSales\n",
    "\n",
    "storeSales = getSalesByStore() # Very slow\n",
    "    \n",
    "DMASpendingPerCapita = {}\n",
    "DMASpending = {}\n",
    "\n",
    "for index,store in storeSales.items():\n",
    "    DMA = mapStoreNumberToDMA(index)\n",
    "    consumerInfoName = mapDMAToName(datasets[\"custom-dma-to-consumerArea.csv\"],DMA)\n",
    "    if type(consumerInfoName) != float:\n",
    "        medianStoreSales = stats.median(storeSales[index])\n",
    "        if DMA not in DMASpendingPerCapita:\n",
    "            DMASpendingPerCapita[DMA] = []\n",
    "            DMASpending[DMA] = []\n",
    "\n",
    "        DMASpendingPerCapita[DMA].append(medianStoreSales)\n",
    "\n",
    "for index,listOfSales in DMASpendingPerCapita.items():\n",
    "    sum = 0\n",
    "    for sale in listOfSales:\n",
    "        sum+=sale\n",
    "    \n",
    "    consumerInfoName = mapDMAToName(datasets[\"custom-dma-to-consumerArea.csv\"],index)\n",
    "    DMASpending[index] = sum\n",
    "    DMASpendingPerCapita[index] = sum/areas[consumerInfoName]['Number of consumer units (in thousands)']\n",
    "\n",
    "\n",
    "print(\"Rankings of Overall Spending (Sum of Medians of Stores)\")\n",
    "for index,value in rankDictionary(DMASpending).items():\n",
    "    print(index + \" : \" + str(value))\n",
    "print()\n",
    "print(\"Rankings of Spending per Capita (Sum of Medians of Stores/Number of Customer Units)\")\n",
    "for index,value in rankDictionary(DMASpendingPerCapita).items():\n",
    "    print(index + \" : \" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no truth data for which market is the best, we will turn to K-Means, an unsupervised machine learning model to separate our features dataset into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersToTry = [2,3,4]\n",
    "\n",
    "featuresSetForAnalysis = datasets[\"Features data set.csv\"].drop(['IsHoliday','Date'],axis=1)\n",
    "\n",
    "def runKMeans(data,numOfClusters):\n",
    "    kmeans = KMeans(n_clusters=numOfClusters)\n",
    "\n",
    "    kmeans.fit(data)\n",
    "    y_km = kmeans.fit_predict(data)\n",
    "    return y_km\n",
    "\n",
    "for clusterNum in clustersToTry:\n",
    "    exportPath = 'export\\Features data set.csv\\\\' + str(clusterNum) + '.csv'\n",
    "    datasetToExport = datasets[\"Features data set.csv\"].assign(cluster = runKMeans(featuresSetForAnalysis,clusterNum).tolist())\n",
    "    datasetToExport.to_csv(exportPath,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
