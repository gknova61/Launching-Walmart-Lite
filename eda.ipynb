{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Walmart Lite - Exploratory Data Analysis\n",
    "\n",
    "Main Objective: Perform EDA on necessary datasets and clean them for analysis.\n",
    "\n",
    "For the Features data set.csv and sales data-set.csv. For other dataset files such as: west.xlsx, south.xlsx, norteast.xlsx, etc. they are small enough to where manual review was sufficient.\n",
    "\n",
    "Before the analysis, we need to get a basic understanding of our datasets being used. That is the objective of this EDA\n",
    "\n",
    "First, we'll import the necessary libraries and read the dataset files into DataFrames that Python/Pandas can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ntpath\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Purpose: Read in dataset files into pandas DataFrame\n",
    "datasets = {}\n",
    "files = []\n",
    "\n",
    "def loadFile(path):\n",
    "    global files\n",
    "    global datasets\n",
    "    \n",
    "    fileExists = os.path.isfile(path)\n",
    "    if not fileExists:\n",
    "        return False\n",
    "    \n",
    "    files.append(path)\n",
    "    \n",
    "    filename,fileExtension = os.path.splitext(path)\n",
    "    \n",
    "    if '.csv' == fileExtension:\n",
    "        df = pd.read_csv(path)\n",
    "    elif '.xlsx' == fileExtension:\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    datasets[ntpath.basename(path)] = df\n",
    "    return True\n",
    "\n",
    "loadFile(\"datasets\\Features data set.csv\")\n",
    "loadFile(\"datasets\\sales data-set.csv\")\n",
    "loadFile(\"datasets\\stores data-set-withDMA.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note there are more datasets, but they are not useful for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are summary statistics for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features data set.csv\n",
      "             Store  Temperature   Fuel_Price      MarkDown1      MarkDown2  \\\n",
      "count  8190.000000  8190.000000  8190.000000    4032.000000    2921.000000   \n",
      "mean     23.000000    59.356198     3.405992    7032.371786    3384.176594   \n",
      "std      12.987966    18.678607     0.431337    9262.747448    8793.583016   \n",
      "min       1.000000    -7.290000     2.472000   -2781.450000    -265.760000   \n",
      "25%      12.000000    45.902500     3.041000    1577.532500      68.880000   \n",
      "50%      23.000000    60.710000     3.513000    4743.580000     364.570000   \n",
      "75%      34.000000    73.880000     3.743000    8923.310000    2153.350000   \n",
      "max      45.000000   101.950000     4.468000  103184.980000  104519.540000   \n",
      "\n",
      "           MarkDown3     MarkDown4      MarkDown5          CPI  Unemployment  \n",
      "count    3613.000000   3464.000000    4050.000000  7605.000000   7605.000000  \n",
      "mean     1760.100180   3292.935886    4132.216422   172.460809      7.826821  \n",
      "std     11276.462208   6792.329861   13086.690278    39.738346      1.877259  \n",
      "min      -179.260000      0.220000    -185.170000   126.064000      3.684000  \n",
      "25%         6.600000    304.687500    1440.827500   132.364839      6.634000  \n",
      "50%        36.260000   1176.425000    2727.135000   182.764003      7.806000  \n",
      "75%       163.150000   3310.007500    4832.555000   213.932412      8.567000  \n",
      "max    149483.310000  67474.850000  771448.100000   228.976456     14.313000  \n",
      "Dimensions: 8190x12\n",
      "\n",
      "sales data-set.csv\n",
      "               Store           Dept   Weekly_Sales\n",
      "count  421570.000000  421570.000000  421570.000000\n",
      "mean       22.200546      44.260317   15981.258123\n",
      "std        12.785297      30.492054   22711.183519\n",
      "min         1.000000       1.000000   -4988.940000\n",
      "25%        11.000000      18.000000    2079.650000\n",
      "50%        22.000000      37.000000    7612.030000\n",
      "75%        33.000000      74.000000   20205.852500\n",
      "max        45.000000      99.000000  693099.360000\n",
      "Dimensions: 421570x5\n",
      "\n",
      "stores data-set-withDMA.xlsx\n",
      "Dimensions: 45x4\n"
     ]
    }
   ],
   "source": [
    "#Purpose: Basic summary of datasets for analysis\n",
    "\n",
    "print(\"Features data set.csv\")\n",
    "print(datasets[\"Features data set.csv\"].describe())\n",
    "print(\"Dimensions: \" + str(datasets[\"Features data set.csv\"].shape[0]) + \"x\" + str(datasets[\"Features data set.csv\"].shape[1]))\n",
    "print()\n",
    "print(\"sales data-set.csv\")\n",
    "print(datasets[\"sales data-set.csv\"].describe())\n",
    "print(\"Dimensions: \" + str(datasets[\"sales data-set.csv\"].shape[0]) + \"x\" + str(datasets[\"sales data-set.csv\"].shape[1]))\n",
    "print()\n",
    "print(\"stores data-set-withDMA.xlsx\")\n",
    "print(\"Dimensions: \" + str(datasets[\"stores data-set-withDMA.xlsx\"].shape[0]) + \"x\" + str(datasets[\"stores data-set-withDMA.xlsx\"].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features dataset, we can see a large amount of missing values for all markdown columns, as well as about 7% (585) of CPI and unemployment columns with missing values. \n",
    "\n",
    "For the sales dataset, there are no missing values.\n",
    "\n",
    "The stores data-set-withDMA contains a mapping of each store to their type, size, and DMA. No further analysis on this dataset needed (nothing missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values\n",
    "\n",
    "###### Markdown Columns\n",
    "\n",
    "In looking at the summary of how much markdown data we have, we've decided that there are too many missing values for these columns to be useful and for our analysis to be uniform across all rows. Therefore, they will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features data set.csv (updated)\n",
      "             Store  Temperature   Fuel_Price          CPI  Unemployment\n",
      "count  8190.000000  8190.000000  8190.000000  7605.000000   7605.000000\n",
      "mean     23.000000    59.356198     3.405992   172.460809      7.826821\n",
      "std      12.987966    18.678607     0.431337    39.738346      1.877259\n",
      "min       1.000000    -7.290000     2.472000   126.064000      3.684000\n",
      "25%      12.000000    45.902500     3.041000   132.364839      6.634000\n",
      "50%      23.000000    60.710000     3.513000   182.764003      7.806000\n",
      "75%      34.000000    73.880000     3.743000   213.932412      8.567000\n",
      "max      45.000000   101.950000     4.468000   228.976456     14.313000\n"
     ]
    }
   ],
   "source": [
    "markDownColumnNames = []\n",
    "for i in range(1,6):\n",
    "    markDownColumnNames.append('MarkDown' + str(i))\n",
    "\n",
    "datasets[\"Features data set.csv\"] = datasets[\"Features data set.csv\"].drop(markDownColumnNames,axis=1)\n",
    "\n",
    "print(\"Features data set.csv (updated)\")\n",
    "print(datasets[\"Features data set.csv\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CPI and Unemployment\n",
    "\n",
    "Since CPI and unemployment rates are calculated on a monthly basis, we will get the months, as well as the store numbers (which later maps to an area) containing missing values. This will tell us how many unique missing values for both metrics we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'05/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], '06/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], '07/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]}\n",
      "\n",
      "{'05/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], '06/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], '07/2013': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]}\n",
      "\n",
      "Are the rows missing a CPI value, also missing an Unemployment value? True\n",
      "Amount of unique missing CPIs: 135\n",
      "Amount of unique missing Unemployment rates: 135\n",
      "Total unique missing values: 270\n"
     ]
    }
   ],
   "source": [
    "#Purpose: Get the dates and areas (store numbers) at which there are missing CPI and unemployment values\n",
    "\n",
    "#Get the row indicies at which there is a missing value\n",
    "def getNAIndexes(data):\n",
    "    i=0\n",
    "    NAIndexes = []\n",
    "\n",
    "    for row in pd.isna(data):\n",
    "        if row == True:\n",
    "            NAIndexes.append(i)\n",
    "        i+=1\n",
    "    return NAIndexes\n",
    "\n",
    "#Given the missing row indicies, return a dictionary containing key:list(storeNumbers)\n",
    "def getNAInfo(data,indexes):\n",
    "    NAInfo = {}\n",
    "    for index in indexes:\n",
    "        day,month,year = data[index:index+1][\"Date\"].values[0].split('/')\n",
    "        date = month + \"/\" + year\n",
    "        store = data[index:index+1][\"Store\"].values[0]\n",
    "        if date not in NAInfo.keys():\n",
    "            NAInfo[date] = []\n",
    "        if store not in NAInfo[date]:\n",
    "            NAInfo[date].append(store)\n",
    "    return NAInfo\n",
    "\n",
    "#Get the length of a dictionary of lists\n",
    "def lenDictOfLists(dictToCount):\n",
    "    count = 0\n",
    "    for listToCount in dictToCount.values():\n",
    "        count += len(listToCount)\n",
    "    return count\n",
    "\n",
    "missingCPIInfo = getNAInfo(datasets[\"Features data set.csv\"],getNAIndexes(datasets[\"Features data set.csv\"][\"CPI\"]))\n",
    "missingUnemploymentInfo = getNAInfo(datasets[\"Features data set.csv\"],getNAIndexes(datasets[\"Features data set.csv\"][\"CPI\"]))\n",
    "\n",
    "print(missingCPIInfo)\n",
    "print()\n",
    "print(missingUnemploymentInfo)\n",
    "print()\n",
    "print(\"Are the rows missing a CPI value, also missing an Unemployment value? \" + str(missingCPIInfo == missingUnemploymentInfo))\n",
    "print(\"Amount of unique missing CPIs: \" + str(lenDictOfLists(missingCPIInfo)))\n",
    "print(\"Amount of unique missing Unemployment rates: \" + str(lenDictOfLists(missingUnemploymentInfo)))\n",
    "print(\"Total unique missing values: \" + str((lenDictOfLists(missingCPIInfo) + lenDictOfLists(missingUnemploymentInfo))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could simply drop rows with missing values, but that would give us less data to work with (585 rows and 3 months less) and that is unacceptable. We could also interpolate the missing values from the nearest date from the same store without missing CPI and unemployment info, but that wouldn't be entirely accurate, either.\n",
    "\n",
    "Since this data is available online, we will aggregate most of the correct rates from here:\n",
    "CPI Dataset - https://www.kaggle.com/bls/consumer-price-index\n",
    "Unemployment Dataset - https://www.kaggle.com/jayrav13/unemployment-by-county-us/data\n",
    "\n",
    "However, the above datasets also have missing states and/or months we need. Therefore, we used the [Bureau of Labor and Statistics Data Viewer]([https://beta.bls.gov/dataViewer/view/timeseries/LAUCN060730000000003]) for the remainder of the missing Unemployment Rates and manually put them into a CSV file (custom-dma-to-unemploymentCounty.csv)\n",
    "\n",
    "For CPI Indexes that were still missing from the above datasets for various reasons talked about in the writeup, we used a combination of annual CPIs for the county or region associated with the DMA and put them into a CSV file (custom-dma-to-cpiMissingValuesYearly.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded external datasets...\n",
      "Building dictionary of missing values to fill in...\n",
      "Success\n",
      "Filling in missing values in Features data set...\n",
      "NOTE: This will only modify the dataset in memory, not the actual CSV file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:172: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CPI and Unemployment missing values have been filled\n",
      "File exported to datasets\\modified\\Features data set.csv\n"
     ]
    }
   ],
   "source": [
    "#Purpose: Using the list of missing values generated above, find the actual missing values from external datasets and export the final features dataset to a CSV file in /datasets/modified\n",
    "\n",
    "monthNumberToMonthName = {\n",
    "    1 : \"January\",\n",
    "    2 : \"February\",\n",
    "    3 : \"March\",\n",
    "    4 : \"April\",\n",
    "    5 : \"May\",\n",
    "    6 : \"June\",\n",
    "    7 : \"July\",\n",
    "    8 : \"August\",\n",
    "    9 : \"September\",\n",
    "    10 : \"October\",\n",
    "    11 : \"November\",\n",
    "    12 : \"December\"\n",
    "}\n",
    "    \n",
    "def getColumnIndex(dataFrame,searchTerm,isExactMatch = False): #Return column index number given column name in dataFrame\n",
    "    i=-1\n",
    "    for columnName in list(dataFrame):\n",
    "        if(isExactMatch):\n",
    "            condition = searchTerm == columnName\n",
    "        else:\n",
    "            condition = searchTerm in columnName\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        if condition == True:\n",
    "            return i\n",
    "    \n",
    "    \n",
    "def buildMissingCPIValues(debug=False): # Build a dictionary containing all missing CPI values from external datasets\n",
    "    global datasets\n",
    "    global missingCPIInfo\n",
    "    \n",
    "    missingValues = {}\n",
    "    missingValueErrors = []\n",
    "    success = True\n",
    "    \n",
    "    for date,stores in missingCPIInfo.items():\n",
    "        for store in stores:\n",
    "            month,year = date.split('/')\n",
    "            month = \"M\" + str(int(month)).zfill(2)\n",
    "            store = int(store)\n",
    "            targetStoreRow = datasets[\"stores data-set-withDMA.xlsx\"].query(\"Store == @store\")\n",
    "            targetDMA = targetStoreRow.values[0][getColumnIndex(targetStoreRow,\"DMA\")]\n",
    "\n",
    "            targetCPIMapRow = datasets[\"custom-dma-to-cpiArea.csv\"].query(\"DMA == @targetDMA\")\n",
    "            targetCPIArea = targetCPIMapRow.values[0][getColumnIndex(targetCPIMapRow,\"CPI_AreaCode\")]\n",
    "\n",
    "            missingValueFulfilled = False\n",
    "            firstAttemptRan = False\n",
    "\n",
    "            if(type(targetCPIArea) == float): #DMA has missing values, in which case pull from missing values csv\n",
    "                firstAttemptRan = True\n",
    "                cpiRow = datasets[\"custom-dma-to-cpiMissingValuesYearly.csv\"].query(\"DMA == @targetDMA and year == @year\")\n",
    "                if len(cpiRow) > 0:\n",
    "                    cpi = cpiRow.values[0][getColumnIndex(cpiRow,\"value\")]\n",
    "                    missingValueFulfilled = True\n",
    "\n",
    "            if not missingValueFulfilled and not firstAttemptRan:\n",
    "                cpiRowsAreaFiltered = datasets[\"cu.data.0.Current.csv\"][datasets[\"cu.data.0.Current.csv\"]['series_id'].str.contains(targetCPIArea) == True]\n",
    "                cpiRowsDateFiltered = cpiRowsAreaFiltered.query(\"year == @year & period == @month\")\n",
    "                if(len(cpiRowsDateFiltered) <= 0): # Dates have missing values, in which case pull from mising values CSV\n",
    "                    cpiRow = datasets[\"custom-dma-to-cpiMissingValuesYearly.csv\"].query(\"DMA == @targetDMA and year == @year\")\n",
    "                    if len(cpiRow) > 0:\n",
    "                        cpi = cpiRow.values[0][getColumnIndex(cpiRow,\"value\")]\n",
    "                        missingValueFulfilled = True\n",
    "                else: #average all CPI values with the area code\n",
    "                    count = 0\n",
    "                    sum = 0\n",
    "                    for index,row in cpiRowsDateFiltered.iterrows():\n",
    "                        sum += row['value']\n",
    "                        count+=1\n",
    "                    mean = sum/count\n",
    "                    cpi = mean\n",
    "                    missingValueFulfilled = True\n",
    "\n",
    "            if not missingValueFulfilled:\n",
    "                    missingValueErrors.append([store,date])\n",
    "                    success = False\n",
    "            else:        \n",
    "                missingValues[(store,date)] = cpi\n",
    "                \n",
    "    if(debug and not success):\n",
    "        print(\"ERROR: Could not build full CPI dictionary\")\n",
    "        return missingValueErrors\n",
    "    else:\n",
    "        if(not success):\n",
    "            return success\n",
    "        else:\n",
    "            return missingValues\n",
    "                \n",
    "def buildMissingUnemploymentRates(debug=False): # Build a dictionary containing all missing unemployment values from external datasets\n",
    "    global datasets\n",
    "    global missingUnemploymentInfo\n",
    "    \n",
    "    missingValues = {}\n",
    "    missingValueErrors = []\n",
    "    success = True\n",
    "    \n",
    "    for date,stores in missingUnemploymentInfo.items():\n",
    "        for store in stores:\n",
    "            month,year = date.split('/')\n",
    "            month = monthNumberToMonthName[int(month)]\n",
    "            store = int(store)\n",
    "            targetStoreRow = datasets[\"stores data-set-withDMA.xlsx\"].query(\"Store == @store\")\n",
    "            targetDMA = targetStoreRow.values[0][getColumnIndex(targetStoreRow,\"DMA\")]\n",
    "\n",
    "            targetUnemploymentMapRow = datasets[\"custom-dma-to-unemploymentCounty.csv\"].query(\"DMA == @targetDMA\")\n",
    "            targetState = targetUnemploymentMapRow.values[0][getColumnIndex(targetUnemploymentMapRow,\"UnemploymentState\")]\n",
    "            targetCounty = targetUnemploymentMapRow.values[0][getColumnIndex(targetUnemploymentMapRow,\"UnemploymentCounty\")]\n",
    "\n",
    "            missingValueFulfilled = False\n",
    "            firstAttemptRan = False\n",
    "            \n",
    "            if type(targetCounty) == float: # County has missing values, in which case pull from missing values csv\n",
    "                firstAttemptRan = True\n",
    "                unemploymentRateRow = datasets[\"custom-dma-to-unemploymentMissingValues.csv\"].query(\"DMA == @targetDMA and Year == @year and Month == @month\")\n",
    "                if len(unemploymentRateRow) > 0:\n",
    "                    unemploymentRate = unemploymentRateRow.values[0][getColumnIndex(unemploymentRateRow,\"UnemploymentRate\")]\n",
    "                    missingValueFulfilled = True\n",
    "\n",
    "            if not missingValueFulfilled and not firstAttemptRan:\n",
    "                unemploymentRateRow = datasets[\"output.csv\"].query(\"Year == @year & Month == @month & State == @targetState & County == @targetCounty\")\n",
    "                if(len(unemploymentRateRow > 0)):\n",
    "                    unemploymentRate = unemploymentRateRow.values[0][getColumnIndex(unemploymentRateRow,\"Rate\")]\n",
    "                    missingValueFulfilled = True\n",
    "                else: # Missing values for the dates in the dataset, in which case pull from missing values csv\n",
    "                    unemploymentRateRow = datasets[\"custom-dma-to-unemploymentMissingValues.csv\"].query(\"DMA == @targetDMA and Year == @year and Month == @month\")\n",
    "                    if len(unemploymentRateRow) > 0:\n",
    "                        unemploymentRate = unemploymentRateRow.values[0][getColumnIndex(unemploymentRateRow,\"UnemploymentRate\")]\n",
    "                        missingValueFulfilled = True\n",
    "\n",
    "            if not missingValueFulfilled:\n",
    "                missingValueErrors.append([store,date])\n",
    "                success = False\n",
    "            else:        \n",
    "                missingValues[(store,date)] = unemploymentRate\n",
    "        \n",
    "    if(debug and not success):\n",
    "        print(\"ERROR: Could not build full unemployment dictionary\")\n",
    "        return missingValueErrors\n",
    "    else:\n",
    "        if(not success):\n",
    "            return success\n",
    "        else:\n",
    "            return missingValues\n",
    "        \n",
    "def fillInFeaturesMissingValues(missingUnemploymentValues,missingCPIValues): # Fill in missing values on Features dataset given dictionaries generated from the above\n",
    "    global datasets\n",
    "    \n",
    "    allValuesFilled = True\n",
    "    \n",
    "    #Basic error checking to make sure both arguments are in the correct format\n",
    "    if(type(missingUnemploymentValues) != dict or type(missingCPIValues) != dict):\n",
    "        return False\n",
    "    \n",
    "    for index,row in datasets[\"Features data set.csv\"].iterrows():\n",
    "        day,month,year = row['Date'].split('/')\n",
    "        date = month + '/' + year\n",
    "        store = int(row['Store'])\n",
    "            \n",
    "        if(pd.isnull(row['Unemployment'])):\n",
    "            if((store,date) in missingUnemploymentValues):\n",
    "                datasets[\"Features data set.csv\"].set_value(index,'Unemployment',missingUnemploymentValues[(store,date)])\n",
    "            else:\n",
    "                allValuesFilled = False\n",
    "                \n",
    "        if(pd.isnull(row['CPI'])):\n",
    "            if((store,date) in missingCPIValues):\n",
    "                datasets[\"Features data set.csv\"].set_value(index,'CPI',missingCPIValues[(store,date)])\n",
    "            else:\n",
    "                allValuesFilled = False\n",
    "    \n",
    "    return allValuesFilled\n",
    "        \n",
    "loadFile(\"datasets\\KaggleUnemploymentDataset\\output.csv\")\n",
    "loadFile(\"datasets\\custom\\custom-dma-to-unemploymentCounty.csv\")\n",
    "loadFile(\"datasets\\custom\\custom-dma-to-cpiArea.csv\")\n",
    "loadFile(\"datasets\\custom\\custom-dma-to-unemploymentMissingValues.csv\")\n",
    "loadFile(\"datasets\\custom\\custom-dma-to-cpiMissingValuesYearly.csv\")\n",
    "loadFile(\"datasets\\KaggleCPIDataset\\cu.data.0.Current.csv\")\n",
    "print(\"Loaded external datasets...\")\n",
    "\n",
    "print(\"Building dictionary of missing values to fill in...\")\n",
    "missingUnemploymentValues = buildMissingUnemploymentRates()\n",
    "missingCPIValues = buildMissingCPIValues()\n",
    "    \n",
    "if(type(missingCPIValues)== dict and type(missingUnemploymentValues) == dict ):\n",
    "    print(\"Success\")\n",
    "\n",
    "    print(\"Filling in missing values in Features data set...\")\n",
    "    print(\"NOTE: This will only modify the dataset in memory, not the actual CSV file\")\n",
    "    \n",
    "    fillInFeaturesMissingValues(missingUnemploymentValues,missingCPIValues)\n",
    "    \n",
    "    if len(datasets[\"Features data set.csv\"].index) == datasets[\"Features data set.csv\"].describe()['Unemployment']['count'] and len(datasets[\"Features data set.csv\"].index) == datasets[\"Features data set.csv\"].describe()['CPI']['count']:\n",
    "        print(\"All CPI and Unemployment missing values have been filled\")\n",
    "    else:\n",
    "        print(\"NOTE: Features dataset still has missing values\")\n",
    "else:\n",
    "    print(\"One of the dictionary builds failed, exiting\")\n",
    "\n",
    "#Export modified Features dataset to file\n",
    "exportPath = 'datasets\\modified\\Features data set.csv'\n",
    "datasets[\"Features data set.csv\"].to_csv(exportPath,index=False)\n",
    "print(\"File exported to \" + exportPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new Features dataset with no missing values (located in datasets/Features data set.csv), it's time to dive into the analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
